{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all your libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "faq = None\n",
    "for path in os.listdir():\n",
    "    if path == \"UTDFAQ.csv\":\n",
    "        faq = path\n",
    "    else:\n",
    "        print(\"UTDFAQ.csv not found in current directory\")\n",
    "        faq = \"FAQ Dataset.csv\"\n",
    "print(faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from rich import print\n",
    "\n",
    "faq_dataset = load_dataset(\n",
    "    \"csv\", data_files=faq)\n",
    "print(faq_dataset)\n",
    "## remove all the None values\n",
    "faq_dataset = faq_dataset.filter(lambda x: x['Question'] is not None and x['Answering'] is not None)\n",
    "## reaplace all the word ARC to AccessAbility Resource Center and\n",
    "'''\n",
    "Office location: Administration Building, Room 2.224\n",
    "Email: studentaccess@utdallas.edu (Do not email attachments, upload documents to utd.link/arcupload only.)\n",
    "Phone: (972) 883-2098\n",
    "Fax: Please don’t fax, use utd.link/arcupload\n",
    "Mail: AD 30, 800 West Campbell Rd., Richardson TX 75080\n",
    "'''\n",
    "faq_dataset = faq_dataset.map(lambda x: {'Question': x['Question'].replace('ARC', 'AccessAbility Resource Center'), 'Answering': x['Answering'].replace('ARC', 'AccessAbility Resource Center')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove all the None values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove all the None values\n",
    "faq_dataset = faq_dataset.filter(lambda x: x['Question'] is not None and x['Answering'] is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace all the ARC to Accessible Resource Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all the word UTD to University of Texas at Dallas\n",
    "# replace all the ARC to AccessAbility Resource Center\n",
    "faq_dataset = faq_dataset.map(lambda x: {'Question': x['Question'].replace('UTD', 'University of Texas at Dallas'), 'Answering': x['Answering'].replace('UTD', 'University of Texas at Dallas')})\n",
    "faq_dataset = faq_dataset.map(lambda x: {'Question': x['Question'].replace('ARC', 'AccessAbility Resource Center'), 'Answering': x['Answering'].replace('ARC', 'AccessAbility Resource Center')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Open AI Emebddigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "OPENKEY_API = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "#ORGANIZATION_ID = os.getenv(\"ORGANIZATION_ID\")\n",
    "#openai.organization = ORGANIZATION_ID\n",
    "# get this from top-right dropdown on OpenAI under organization > settings\n",
    "openai.api_key = OPENKEY_API\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENKEY_API\n",
    "openai.Engine.list()  # check we have authenticated\n",
    "print(openai.Engine.list())\n",
    "## model of choices\n",
    "MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ini the Pinecone Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "PINECONE_APIKEY = os.getenv(\"PINECONE_APIKEY\")\n",
    "if PINECONE_APIKEY is None:\n",
    "    raise Exception(\"PINECONE_API_KEY not found in environment variables add the Pinecone API key to the .env file\")\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key = PINECONE_APIKEY,\n",
    "    environment=\"us-east-1-aws\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env():\n",
    "    #setting up OpenAI Api\n",
    "    OPENKEY_API = os.getenv(\"OPENAI_KEY\")\n",
    "    ORGANIZATION_ID = os.getenv(\"ORGANIZATION_ID\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENKEY_API\n",
    "    openai.organization = ORGANIZATION_ID\n",
    "    # get this from top-right dropdown on OpenAI under organization > settings\n",
    "    openai.api_key = OPENKEY_API\n",
    "    # get API key from top-right dropdown on OpenAI website\n",
    "\n",
    "    openai.Engine.list()  # check we have authenticated\n",
    "\n",
    "    #setting pup pinecon\n",
    "    PINECONE_APIKEY = os.getenv(\"PINECONE_APIKEY\")\n",
    "    # initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "    pinecone.init(\n",
    "        api_key = PINECONE_APIKEY ,\n",
    "        environment=\"us-east1-gcp\" , \n",
    "    )\n",
    "    assert len(pinecone.list_indexes()) > 0, \"No indexes found in your Pinecone account. Please create an index and try again.\"\n",
    "\n",
    "    print(pinecone.list_indexes())\n",
    "    if pinecone.list_indexes()[0] != \"utd-chatbot\":\n",
    "        print(\"Please create an index named 'utd-chatbot\")\n",
    "\n",
    "\n",
    "    print(f'The list of pinecone index {pinecone.list_indexes()}')\n",
    "    if pinecone.list_indexes()[0] != \"utd-chatbot\":\n",
    "        print(\"Please create an index named 'utd-chatbot\")\n",
    "\n",
    "    ## I the name of the vector database\n",
    "    return  pinecone.Index(index_name = pinecone.list_indexes()[0]) , pinecone.list_indexes()[0] , pinecone.describe_index(pinecone.list_indexes()[0])\n",
    "\n",
    "pinecone_index , index_name , index_description = set_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt = \"\"\"\n",
    "Please act as a University of Texas at Dallas Counselor. I will provide you with an individual \n",
    "looking for guidance at the University of Texas at Dallas, and your task is to help them \n",
    "solve their problem\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(ans):\n",
    "    if \"ARC\" in ans:\n",
    "        ans = ans.replace(\"ARC\", \"AccessAbility Resource Center\")\n",
    "        ans += \"\"\" \n",
    "        Office location: Administration Building, Room 2.224 \\n\n",
    "        Email: studentaccess@utdallas.edu (Do not email attachments, upload documents to utd.link/arcupload only.) \\n\n",
    "        Phone: (972) 883-2098 \\n\n",
    "        Fax: Please don’t fax, use utd.link/arcupload \\n\n",
    "        Mail: AD 30, 800 West Campbell Rd., Richardson TX 75080    Office location: Administration Building, Room 2.224\\n\n",
    "        \"\"\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pinecone import Pinecone\n",
    "\n",
    "def query_vector_database(query):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # vectorstore = Pinecone.from_documents(documents, embeddings, index_name=INDEX_NAME) # only used for initial index creation, upserts document embeddings as well as the documents themselves\n",
    "    vectorstore = Pinecone( pinecone_index, embeddings.embed_query, \"text\") # use this for subsequent runs\n",
    "\n",
    "    docs = vectorstore.similarity_search_with_score(query)\n",
    "\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "      answer = faq_dataset['train'].filter(lambda x: x['Question'] == doc[0].page_content) # get row with the corresponding question in query\n",
    "      print(answer)\n",
    "      res.append({\"Question\": f\"{answer['Question'][0]}\", \"Answer\": f\"{answer['Answering'][0]}\", \"URL\": f\"{answer['URL'][0]}\"}) # adds a dictionary of the row to list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Pinecone( pinecone_index, embeddings.embed_query, \"text\") # use this for subsequent runs\n",
    "docs = vectorstore.similarity_search_with_score(\"I have ADHD and I need help with my classes.\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "def create_an_standard_qa_prompt(res):\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"Question\", \"Answer\", \"URL\"], \n",
    "        template=\"Question: {Question}\\n{Answer}\\nSource:{URL}\"\n",
    "    )\n",
    "\n",
    "    fewShotPrompt = FewShotPromptTemplate(\n",
    "        examples=res,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "    system_message_prompt = SystemMessage(content=role_prompt)\n",
    "    human_message_prompt = HumanMessagePromptTemplate(prompt=fewShotPrompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    type(chat_prompt)\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "def called_llm_decoder_model(query, prompt):\n",
    "  chain = LLMChain(llm=chat, prompt=prompt)\n",
    "  output = chain.run(input=query)\n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "questions = []\n",
    "answers = []\n",
    "import random\n",
    "random_value = random.randint(0, 1000)\n",
    "\n",
    "def utd_chatbot( question ):\n",
    "    ## store the question in the csvs file\n",
    "    #print(question)\n",
    "    res = query_vector_database(question)\n",
    "    #print(res)\n",
    "    propmt = create_an_standard_qa_prompt(res)\n",
    "    #print(propmt)\n",
    "    ans = called_llm_decoder_model(question, propmt)\n",
    "    # post_process(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lauch of Gradio\n",
    "demo = gr.Interface(fn=utd_chatbot, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch( share = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# Close the a demo\n",
    "#demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Language Chain to Generate one Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember.\n",
    "\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "from rich import print\n",
    "print( llm(\"Tell me about yourself\") ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('utd_chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9ff9b4d34df2539584901dcbbee8740b4a40a3636c7400fd1d2b55da32052e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
