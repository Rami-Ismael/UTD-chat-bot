{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Reddit with Python\n",
    "import praw\n",
    "# Make easy to read\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ge the Python Dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "HUGGINGFACE_TOKEN  = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "REDDIT_CLIENT_ID  = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET  = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME  = os.getenv(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD  = os.getenv(\"REDDIT_PASSWORD\")\n",
    "REDDIT_USER_AGENT  = os.getenv(\"REDDIT_USER_AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Called Reddits API\n",
    "1. User-Agent (string) - A short description of what the bot does. For example: \"Python:Sentdex Sentiment Analysis v0.1 (by /u/sentdex) , scraped_utd_reddit_bot\"\n",
    "2. Client-ID (string) - 14 character key that is generated when you create an app. For example: \"Jxk7Jxk7Jxk7Jxk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits = praw.Reddit(client_id = REDDIT_CLIENT_ID ,\n",
    "                      client_secret =  REDDIT_CLIENT_SECRET ,\n",
    "                      username =  REDDIT_USERNAME ,\n",
    "                      password =  REDDIT_PASSWORD ,\n",
    "                        user_agent = REDDIT_USER_AGENT )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Called a Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddits.subreddit(\"utdallas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The data type of hot_utd is <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'praw.models.listing.generator.ListingGenerator'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The data type of hot_utd is \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'praw.models.listing.generator.ListingGenerator'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> posts on r/utdallas\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top \u001b[1;36m3\u001b[0m posts on r/utdallas\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Life Changing News at the Library\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Life Changing News at the Library\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Papa John’s at the dining hall\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Papa John’s at the dining hall\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Weather for tuesday!! <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Weather for tuesday!! \u001b[1;36m01\u001b[0m/\u001b[1;36m24\u001b[0m/\u001b[1;36m2023\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hot_utd = subreddit.hot(limit = 3)\n",
    "print(f\"The data type of hot_utd is {type(hot_utd)}\")\n",
    "print(\"Top 3 posts on r/utdallas\")\n",
    "for submission in hot_utd:\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all of the submission from utd dallas. \n",
    "1. Get Reddit each Post\n",
    "    1. Commets ( I have to go through the comment forest)\n",
    "        1. get the id of the comment , which are the reply to the original post\n",
    "        2.Go through the comment forest and get the id of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_data(subreddit, subreddit_limit:int = 10 , comment_limit:int = 10 ):\n",
    "    \"\"\"\n",
    "    This function takes in a subreddit and returns a dictionary with the\n",
    "    \"\"\"\n",
    "    all_elements = subreddit.top( limit =  subreddit_limit  , time_filter = \"all\")\n",
    "    ## index \n",
    "    titles = [None] * subreddit_limit\n",
    "    self_texts = [None] * subreddit_limit\n",
    "    index = 0 \n",
    "    ## filter the elements where the selftext is not empt\n",
    "    all_element_dict = dict()\n",
    "    for submission in all_elements:\n",
    "        ## store the id of the submission\n",
    "        id = submission.id\n",
    "        ## create a dictionary for each submission\n",
    "        all_element_dict[ id ] = dict()\n",
    "        ## store the title of the submission\n",
    "        all_element_dict[ id ][\"title\"] = submission.title\n",
    "        titles[ index ] = submission.title\n",
    "        ## store the selftext of the submission\n",
    "        all_element_dict[ id ][\"selftext\"] = submission.selftext\n",
    "        self_texts[ index ] = submission.selftext\n",
    "        index = index + 1\n",
    "        ## store the author of the submission\n",
    "        try:\n",
    "            all_element_dict[ id ][\"author\"] = submission.author.name\n",
    "        except AttributeError as e:\n",
    "            all_element_dict[ id ][\"author\"] = \"unknown\"\n",
    "        ## store the number of comments of the submission\n",
    "        all_element_dict[ id ][\"num_comments\"] = submission.num_comments\n",
    "        ## store the permalink of the submission\n",
    "        all_element_dict[ id ][\"permalink\"] = submission.permalink\n",
    "        ## store the url of the submission\n",
    "        all_element_dict[ id ][\"url\"] = submission.url\n",
    "        ## store all the comments in text format from reddits\n",
    "        all_element_dict[ id ][\"comments\"] = dict()\n",
    "        ## get all the comments (including the replies to the comments)\n",
    "        submission.comments.replace_more( limit = comment_limit )\n",
    "        ## create an function with a yield similar to a dfs\n",
    "        for comment in submission.comments.list():\n",
    "            try:\n",
    "                ## store the id of the comment\n",
    "                if comment.id not in all_element_dict[ id ][\"comments\"]:\n",
    "                    all_element_dict[ id ][\"comments\"][ comment.id ] = dict()\n",
    "                ## store the body of the comment\n",
    "                all_element_dict[ id ][\"comments\"][ comment.id ][\"body\"] = comment.body\n",
    "                ## store the author of the comment\n",
    "                try:\n",
    "                    all_element_dict[ id ][\"comments\"][ comment.id ][\"author\"] = comment.author.name\n",
    "                except:\n",
    "                    all_element_dict[ id ][\"comments\"][ comment.id ][\"author\"] = \"unknown\"\n",
    "                    \n",
    "                ## store the score of the comment\n",
    "                all_element_dict[ id ][\"comments\"][ comment.id ][\"score\"] = comment.score\n",
    "                ## store the replies to the comments\n",
    "                if comment.replies != None and len(comment.replies) > 0 :\n",
    "                    all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"] = dict()\n",
    "                ## store the replies to the comments\n",
    "                for reply in comment.replies:\n",
    "                    try:\n",
    "                        ## store the id of the reply\n",
    "                        if reply.id not in all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"]:\n",
    "                            all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"][ reply.id ] = dict()\n",
    "                        ## store the body of the reply\n",
    "                        all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"][ reply.id ][\"body\"] = reply.body\n",
    "                        ## store the author of the reply\n",
    "                        try:\n",
    "                            all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"][ reply.id ][\"author\"] = reply.author.name\n",
    "                        except:\n",
    "                            all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"][ reply.id ][\"author\"] = \"unknown\"\n",
    "                        ## store the score of the reply\n",
    "                        all_element_dict[ id ][\"comments\"][ comment.id ][\"replies\"][ reply.id ][\"score\"] = reply.score\n",
    "                    ## there error caomes from the called NonType object has no attributes\n",
    "                    except AttributeError as e:\n",
    "                        print(f\"The author of the reply is {reply.author}\")\n",
    "                        print(e)\n",
    "                        break\n",
    "            except:\n",
    "                print(f\"The author of the comment is {comment.author}\")\n",
    "                print(e)\n",
    "                break\n",
    "            \n",
    "        ## tree of the comments\n",
    "    return all_element_dict , titles , self_texts\n",
    "all_element_dict   , titles , self_texts= get_reddit_data( subreddit = subreddit, subreddit_limit = 100 , comment_limit = 100 )\n",
    "#print(all_element_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dictionary to a json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The size of the dictionary is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The size of the dictionary is \u001b[1;36m100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the dictionary to a json files names utd_reddit.json\n",
    "import json\n",
    "## get the size of the dictionary\n",
    "print(f\"The size of the dictionary is {len(all_element_dict)}\")\n",
    "with open(\"utd_reddit.json\", \"w\") as f:\n",
    "    json.dump(all_element_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the list into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                               title  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>               Not all restaurants are equal at UTD   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  Yeah so I turned the default UTD campus pictur<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                   UTD's next semester plan be like   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                                     Welcome to UTD   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                              We are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>% safe guys   \n",
       "\n",
       "                                            selftext  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                                     \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  &amp;#x200B;\\n\\n<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://preview.redd.it/f3axixdjzv...</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                     \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                                                     \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "\u001b[1;36m0\u001b[0m               Not all restaurants are equal at UTD   \n",
       "\u001b[1;36m1\u001b[0m  Yeah so I turned the default UTD campus pictur\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m                   UTD's next semester plan be like   \n",
       "\u001b[1;36m3\u001b[0m                                     Welcome to UTD   \n",
       "\u001b[1;36m4\u001b[0m                              We are \u001b[1;36m100\u001b[0m% safe guys   \n",
       "\n",
       "                                            selftext  \n",
       "\u001b[1;36m0\u001b[0m                                                     \n",
       "\u001b[1;36m1\u001b[0m  &#x200B;\\n\\n\u001b[4;94mhttps://preview.redd.it/f3axixdjzv...\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m                                                     \n",
       "\u001b[1;36m3\u001b[0m                                                     \n",
       "\u001b[1;36m4\u001b[0m                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Convert the two list into a panda dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"title\":titles, \"selftext\":self_texts})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Panda DatFrame to a Hugging Face Dataset and Push to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "datatset_pd = Dataset.from_pandas(df)\n",
    "## upload the dataset to huggingface\n",
    "datatset_pd.upload_dataset(\"utd_reddit_pd\" ,token  = HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Json file in to Dataset Dictionary a librarues create by Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-185618a055c924a1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/null/.cache/huggingface/datasets/json/default-185618a055c924a1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1868.29it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 197.26it/s]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/null/.cache/huggingface/datasets/json/default-185618a055c924a1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datatset = load_dataset(\"json\", data_files = \"utd_reddit.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Dataset to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:04<00:00,  4.64s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "https://huggingface.co/datasets/Rami/utd_reddit/resolve/main/data/train-00000-of-00001-5025aa51fdffbdb4.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36m_info\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                     )\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36m_file_info\u001b[0;34m(url, session, size_policy, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m             )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 403, message='Forbidden', url=URL('https://cdn-lfs.huggingface.co/repos/58/ef/58ef2f83a20d48ea51fdca8f46ce2c96a482ab18419ef482faa77e778531e2ea/106a59a6221d80b16430a3611c1d38dfe04316089eb97d6358c6ef6fb38ad740?response-content-disposition=attachment%3B+filename*%3DUTF-8''train-00000-of-00001-5025aa51fdffbdb4.parquet%3B+filename%3D%22train-00000-of-00001-5025aa51fdffbdb4.parquet%22%3B&Expires=1674838632&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2VmLzU4ZWYyZjgzYTIwZDQ4ZWE1MWZkY2E4ZjQ2Y2UyYzk2YTQ4MmFiMTg0MTllZjQ4MmZhYTc3ZTc3ODUzMWUyZWEvMTA2YTU5YTYyMjFkODBiMTY0MzBhMzYxMWMxZDM4ZGZlMDQzMTYwODllYjk3ZDYzNThjNmVmNmZiMzhhZDc0MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IrZmlsZW5hbWUqJTNEVVRGLTglMjclMjd0cmFpbi0wMDAwMC1vZi0wMDAwMS01MDI1YWE1MWZkZmZiZGI0LnBhcnF1ZXQlM0IrZmlsZW5hbWUlM0QlMjJ0cmFpbi0wMDAwMC1vZi0wMDAwMS01MDI1YWE1MWZkZmZiZGI0LnBhcnF1ZXQlMjIlM0IiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzQ4Mzg2MzJ9fX1dfQ__&Signature=dOjYAy-Jn1LD0ARXiD3JAq~rDXf~4Nv5ZIcuezRhmf7IGus5mNub4msszsGT~drSbry2MeRdEWiW-3CT58x5TZT0RZyl-4cvhEmAucSm7SMo~Q1jP0Uv-oBaWcucXqV~c6yCRKeIyypPom0nXVH1qRcMvke7RtohQv0xN28sLFMg-b5H1EsPMBNHnqrIevPQmgEGeEqAPgl26-8yuVkecKQnqaUih4rc5cYrYl1OaO5KFKP9cKkLZWkI3gDyokzAyBxNew9f-5NTIFl7ORC1IXwZxxbEB05RzEoiCWkRFNmrgWEy5oxdZhvY0O9dVjgh76zrdf4KDVn2xvJBja237Q__&Key-Pair-Id=KVTP0A1DKRTAX')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25748/583306703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatatset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utd_reddit\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, private, token, branch, max_shard_size, num_shards, shard_size, embed_external_files)\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_shard_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                 \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_shards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m                 \u001b[0membed_external_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_external_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m             )\n\u001b[1;32m   1558\u001b[0m             \u001b[0mtotal_uploaded_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0muploaded_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   4838\u001b[0m         ]\n\u001b[1;32m   4839\u001b[0m         deleted_size = sum(\n\u001b[0;32m-> 4840\u001b[0;31m             \u001b[0mxgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_hub_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files_to_delete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4841\u001b[0m         )\n\u001b[1;32m   4842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4838\u001b[0m         ]\n\u001b[1;32m   4839\u001b[0m         deleted_size = sum(\n\u001b[0;32m-> 4840\u001b[0;31m             \u001b[0mxgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_hub_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files_to_delete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4841\u001b[0m         )\n\u001b[1;32m   4842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/datasets/download/streaming_download_manager.py\u001b[0m in \u001b[0;36mxgetsize\u001b[0;34m(path, use_auth_token)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fs_token_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_hop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;31m# use xopen instead of fs.open to make data fetching more robust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFSTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcoro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_size\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/utd_chatbot/lib/python3.7/site-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36m_info\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;31m# If get failed, then raise a FileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: https://huggingface.co/datasets/Rami/utd_reddit/resolve/main/data/train-00000-of-00001-5025aa51fdffbdb4.parquet"
     ]
    }
   ],
   "source": [
    "datatset.push_to_hub(\"utd_reddit\" , token = HUGGINGFACE_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('utd_chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9ff9b4d34df2539584901dcbbee8740b4a40a3636c7400fd1d2b55da32052e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
