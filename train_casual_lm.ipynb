{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the latest version of Hugging Face's Transformers library with Pytorch support to train a casual language model for dialogue generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if the code is running on a google colab environment\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    # Adding making a notebook experience better\n",
    "    !pip install rich\n",
    "    \n",
    "    # Macine Learning Libraries\n",
    "    \n",
    "    ## Deep Learning  \n",
    "    \n",
    "    ### Deep Learning Frameworks of Choices\n",
    "    !pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
    "\n",
    "    \n",
    "    ## Deep Learning Learning for NLP \n",
    "    \n",
    "    ### Premade and Built Deep Learning Models\n",
    "    !pip install transformers\n",
    "    \n",
    "# Adding making a notebook experience better\n",
    "from rich import print\n",
    "\n",
    "## Machine Learning Libraries\n",
    "\n",
    "### Machine Learning for NLP\n",
    "\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Single GPU Training ( I am Broke )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If I have a GPU set the default data type to tf32 which is faster than fp32 and only part of cuda gpu\n",
    "The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total.\n",
    "It’s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:\n",
    "- When this is done CUDA will automatically switch to using tf32 instead of fp32 where it’s possible. This, of course, assumes that the used GPU is from the Ampere series.\n",
    "- Note: tf32 mode is internal to CUDA and can’t be accessed directly via tensor.to(dtype=torch.tf32) as torch.tf32 doesn’t exist.\n",
    "- Note: you need torch>=1.7 to enjoy this feature.\n",
    "- You can also see a variety of benchmarks on tf32 vs other precisions: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\n",
    "- We’ve now seen how we can change the floating types to increase throughput, but we are not done, yet! There is another area where we can save GPU memory: the optimizer.[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if the PyTorch version is equal or greater than 1.7.0\n",
    "import torch\n",
    "if torch.__version__ > '1.7.0':\n",
    "    print('PyTorch version is updated to 1.7.0')\n",
    "    import torch\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "else:\n",
    "    print('PyTorch version is not updated to 1.7.0 and is {torch.__version__} and strongly recommended to update it to 1.7.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Dataset from Hugging Face's Datasets Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Scaling Instruction-Finetuned Language Models\n",
    "By Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, Jason Wei Container: arXiv.org Year: 2022 DOI: 10.48550/arXiv.2210.11416 URL: https://arxiv.org/abs/2210.11416\n",
    "\n",
    "[2]LoRA: Low-Rank Adaptation of Large Language Models\n",
    "By Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen Container: arXiv.org Year: 2021 DOI: 10.48550/arXiv.2106.09685 URL: https://arxiv.org/abs/2106.09685\n",
    "\n",
    "[3]8-bit Optimizers via Block-wise Quantization\n",
    "By Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer Container: arXiv.org Year: 2021 DOI: 10.48550/arXiv.2110.02861 URL: https://arxiv.org/abs/2110.02861\n",
    "[3]8-bit Optimizers via Block-wise Quantization\n",
    "By Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer Container: arXiv.org Year: 2021 DOI: 10.48550/arXiv.2110.02861 URL: https://arxiv.org/abs/2110.02861\n",
    "\n",
    "[4]MQBench: Towards Reproducible and Deployable Model Quantization Benchmark\n",
    "By Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao Gong, Fengwei Yu, Junjie Yan Container: arXiv.org Year: 2021 DOI: 10.48550/arXiv.2111.03759 URL: https://arxiv.org/abs/2111.03759\n",
    "\n",
    "[5]InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval\n",
    "By Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, Rodrigo Nogueira Container: arXiv.org Year: 2023 DOI: 10.48550/arXiv.2301.01820 URL: https://arxiv.org/abs/2301.01820v2\n",
    "\n",
    "\n",
    "[6]@inproceedings{dao2022flashattention,\n",
    "  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\n",
    "  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[7]https://www.pinecone.io/learn/question-answering/\n",
    "\n",
    "[8]https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/\n",
    "\n",
    "[9] https://huggingface.co/docs/transformers/perf_train_gpu_one#bf16\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
